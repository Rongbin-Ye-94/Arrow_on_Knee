{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b62ab8df-43b2-475b-b96c-fa58f1c0d6b1",
   "metadata": {},
   "source": [
    "# Knee K-S Score Detection\n",
    "This Notebook utilize the information of Knee \n",
    "### Author: Jiaqi Chen & Rongbin Ye \n",
    "### Date: 05/18/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dab6baf-6d73-46cd-9c2b-4bad053c9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "import pandas as pd\n",
    "from torchmetrics import Precision, Recall, F1Score\n",
    "import json\n",
    "\n",
    "## Loading the data using the existing tools\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "919034dd-2fed-43ae-9207-9eb6adcf34da",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up logging configuration\n",
    "def setup_logging():\n",
    "    # Create logs directory if it doesn't exist\n",
    "    if not os.path.exists('logs'):\n",
    "        os.makedirs('logs')\n",
    "    \n",
    "    # Create a timestamp for the log file\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = f'logs/data_loading_{timestamp}.log'\n",
    "    \n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()  # This will also print to console\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "# Initialize logger\n",
    "logger = setup_logging()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c79c284-7ff4-448f-bcfe-189b15e939e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:45:46,140 - INFO - Expert 1 Dataset loaded successfully\n",
      "2025-05-18 17:45:46,141 - INFO - Expert 1 classes: ['0Normal', '1Doubtful', '2Mild', '3Moderate', '4Severe']\n",
      "2025-05-18 17:45:46,142 - INFO - Expert 1 class to index mapping: {'0Normal': 0, '1Doubtful': 1, '2Mild': 2, '3Moderate': 3, '4Severe': 4}\n",
      "2025-05-18 17:45:46,143 - INFO - Expert 1 total samples: 1650\n",
      "2025-05-18 17:45:46,154 - INFO - Expert 2 Dataset loaded successfully\n",
      "2025-05-18 17:45:46,154 - INFO - Expert 2 classes: ['0Normal', '1Doubtful', '2Mild', '3Moderate', '4Severe']\n",
      "2025-05-18 17:45:46,154 - INFO - Expert 2 class to index mapping: {'0Normal': 0, '1Doubtful': 1, '2Mild': 2, '3Moderate': 3, '4Severe': 4}\n",
      "2025-05-18 17:45:46,155 - INFO - Expert 2 total samples: 1650\n",
      "2025-05-18 17:45:46,155 - INFO - \n",
      "Medical Expert 1 Dataset Information:\n",
      "2025-05-18 17:45:46,155 - INFO - -------------------------\n",
      "2025-05-18 17:45:46,172 - INFO - Sample image shape: torch.Size([3, 128, 128])\n",
      "2025-05-18 17:45:46,173 - INFO - Sample label: 0 (Class: 0Normal)\n",
      "2025-05-18 17:45:48,888 - INFO - \n",
      "Class distribution:\n",
      "2025-05-18 17:45:48,888 - INFO - 0Normal: 514 images\n",
      "2025-05-18 17:45:48,888 - INFO - 1Doubtful: 477 images\n",
      "2025-05-18 17:45:48,889 - INFO - 2Mild: 232 images\n",
      "2025-05-18 17:45:48,889 - INFO - 3Moderate: 221 images\n",
      "2025-05-18 17:45:48,889 - INFO - 4Severe: 206 images\n",
      "2025-05-18 17:45:48,890 - INFO - \n",
      "Medical Expert 2 Dataset Information:\n",
      "2025-05-18 17:45:48,890 - INFO - -------------------------\n",
      "2025-05-18 17:45:48,892 - INFO - Sample image shape: torch.Size([3, 128, 128])\n",
      "2025-05-18 17:45:48,892 - INFO - Sample label: 0 (Class: 0Normal)\n",
      "2025-05-18 17:45:51,659 - INFO - \n",
      "Class distribution:\n",
      "2025-05-18 17:45:51,660 - INFO - 0Normal: 503 images\n",
      "2025-05-18 17:45:51,660 - INFO - 1Doubtful: 488 images\n",
      "2025-05-18 17:45:51,660 - INFO - 2Mild: 232 images\n",
      "2025-05-18 17:45:51,660 - INFO - 3Moderate: 221 images\n",
      "2025-05-18 17:45:51,661 - INFO - 4Severe: 206 images\n",
      "2025-05-18 17:45:51,661 - INFO - \n",
      "Both experts use the same class labels\n"
     ]
    }
   ],
   "source": [
    "# Define the transforms\n",
    "## Transformation based on the X-ray:\n",
    "### this step will add some additional Noises/distorition\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Base path for the dataset\n",
    "base_path = \"/Users/mega_potato/Downloads/Side_Project/CNN_Toy/data/Digital Knee X-ray Images/Digital Knee X-ray Images/Knee X-ray Images\"\n",
    "\n",
    "# Paths for both expert assessments\n",
    "expert1_path = os.path.join(base_path, \"MedicalExpert-I/MedicalExpert-I\")\n",
    "expert2_path = os.path.join(base_path, \"MedicalExpert-II/MedicalExpert-II\")\n",
    "\n",
    "# Load both datasets\n",
    "try:\n",
    "    dataset_expert1 = ImageFolder(\n",
    "        expert1_path,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    logger.info(f\"Expert 1 Dataset loaded successfully\")\n",
    "    logger.info(f\"Expert 1 classes: {dataset_expert1.classes}\")\n",
    "    logger.info(f\"Expert 1 class to index mapping: {dataset_expert1.class_to_idx}\")\n",
    "    logger.info(f\"Expert 1 total samples: {len(dataset_expert1)}\")\n",
    "\n",
    "    dataset_expert2 = ImageFolder(\n",
    "        expert2_path,\n",
    "        transform=train_transform,\n",
    "    )\n",
    "    logger.info(f\"Expert 2 Dataset loaded successfully\")\n",
    "    logger.info(f\"Expert 2 classes: {dataset_expert2.classes}\")\n",
    "    logger.info(f\"Expert 2 class to index mapping: {dataset_expert2.class_to_idx}\")\n",
    "    logger.info(f\"Expert 2 total samples: {len(dataset_expert2)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading datasets: {str(e)}\")\n",
    "\n",
    "def print_dataset_info(dataset, expert_name):\n",
    "    \"\"\"Helper function to print and compare dataset information\"\"\"\n",
    "    logger.info(f\"\\n{expert_name} Dataset Information:\")\n",
    "    logger.info(\"-------------------------\")\n",
    "    \n",
    "    # Get sample image and label\n",
    "    image, label = next(iter(dataset))\n",
    "    logger.info(f\"Sample image shape: {image.shape}\")\n",
    "    logger.info(f\"Sample label: {label} (Class: {dataset.classes[label]})\")\n",
    "    \n",
    "    # Get class distribution\n",
    "    class_counts = {dataset.classes[i]: 0 for i in range(len(dataset.classes))}\n",
    "    for _, label in dataset:\n",
    "        class_counts[dataset.classes[label]] += 1\n",
    "    \n",
    "    logger.info(\"\\nClass distribution:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        logger.info(f\"{class_name}: {count} images\")\n",
    "\n",
    "# Print information for both datasets\n",
    "print_dataset_info(dataset_expert1, \"Medical Expert 1\")\n",
    "print_dataset_info(dataset_expert2, \"Medical Expert 2\")\n",
    "\n",
    "# Optional: Check if the class labels match between experts\n",
    "if dataset_expert1.classes == dataset_expert2.classes:\n",
    "    logger.info(\"\\nBoth experts use the same class labels\")\n",
    "else:\n",
    "    logger.info(\"\\nWarning: Experts use different class labels:\")\n",
    "    logger.info(f\"Expert 1: {dataset_expert1.classes}\")\n",
    "    logger.info(f\"Expert 2: {dataset_expert2.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55548aa4-9f79-4dc4-a065-eb5958d8abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing module for reading ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28bf5b0-27f4-4b76-9312-dc2ba18b51b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "####### Testing the dataset loading #########################################################\n",
    "def test_dataset_loading(dataset, expert_name):\n",
    "    \"\"\"\n",
    "    Test function to verify dataset loading and display key information\n",
    "    \"\"\"\n",
    "    logger.info(f\"\\nTesting {expert_name} dataset...\")\n",
    "    \n",
    "    # 1. Test dataset size\n",
    "    assert len(dataset) > 0, \"Dataset is empty!\"\n",
    "    logger.info(f\"✓ Dataset contains {len(dataset)} samples\")\n",
    "    \n",
    "    # 2. Test if we can access an image and its label\n",
    "    sample_img, sample_label = dataset[0]\n",
    "    logger.info(f\"✓ First image shape: {sample_img.shape}\")\n",
    "    logger.info(f\"✓ First image label: {dataset.classes[sample_label]}\")\n",
    "    \n",
    "    # 3. Verify image dimensions\n",
    "    assert sample_img.shape == (3, 128, 128), f\"Unexpected image shape: {sample_img.shape}\"\n",
    "    logger.info(\"✓ Image dimensions are correct (3, 128, 128)\")\n",
    "    \n",
    "    # 4. Print class distribution\n",
    "    class_counts = {dataset.classes[i]: 0 for i in range(len(dataset.classes))}\n",
    "    for _, label in dataset:\n",
    "        class_counts[dataset.classes[label]] += 1\n",
    "    \n",
    "    logger.info(\"\\nClass distribution:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        logger.info(f\"{class_name}: {count} images\")\n",
    "    \n",
    "    logger.info(f\"\\n{expert_name} dataset testing completed successfully! ✓\")\n",
    "    return True\n",
    "\n",
    "# Run tests for both datasets\n",
    "try:\n",
    "    test_dataset_loading(dataset_expert1, \"Medical Expert 1\")\n",
    "    test_dataset_loading(dataset_expert2, \"Medical Expert 2\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Dataset testing failed: {str(e)}\")\n",
    "\n",
    "#### END OF TESTING ############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d767e091-156b-4c37-8af1-ff46ac7a5d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Creating the model #####################################################################\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 8, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // 8, in_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.attention(x)\n",
    "        return x * attention_weights\n",
    "\n",
    "class OrdinalRegressionLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weights=None):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # Weights for each boundary between classes\n",
    "        self.weights = weights if weights is not None else torch.ones(num_classes - 1)\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # Convert targets to ordinal encoding\n",
    "        ordinal_targets = torch.zeros(targets.size(0), self.num_classes - 1)\n",
    "        for i in range(targets.size(0)):\n",
    "            ordinal_targets[i, :targets[i]] = 1\n",
    "        \n",
    "        ordinal_targets = ordinal_targets.to(predictions.device)\n",
    "        \n",
    "        # Calculate binary cross entropy for each ordinal level\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            predictions, ordinal_targets, \n",
    "            reduction='none'\n",
    "        )\n",
    "        \n",
    "        # Apply class weights\n",
    "        weighted_loss = loss * self.weights.to(predictions.device)\n",
    "        return weighted_loss.mean()\n",
    "\n",
    "class ImprovedNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution with larger kernel\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers with attention\n",
    "        self.layer1 = nn.Sequential(\n",
    "            ResidualBlock(64, 64),\n",
    "            ResidualBlock(64, 64),\n",
    "            AttentionBlock(64)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, stride=2),\n",
    "            ResidualBlock(128, 128),\n",
    "            AttentionBlock(128)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, stride=2),\n",
    "            ResidualBlock(256, 256),\n",
    "            AttentionBlock(256)\n",
    "        )\n",
    "        \n",
    "        # Global average pooling and classifier\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(256, num_classes - 1)  # One less output for ordinal regression\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc973073-3a79-4b78-9a43-b55d23fc3ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:48:15,508 - INFO - \n",
      "Dataset split:\n",
      "2025-05-18 17:48:15,509 - INFO - Total samples: 1650\n",
      "2025-05-18 17:48:15,509 - INFO - Training samples: 1320\n",
      "2025-05-18 17:48:15,510 - INFO - Test samples: 330\n"
     ]
    }
   ],
   "source": [
    "# For this example, we'll use Expert 1's dataset\n",
    "# Split dataset into train and test sets\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Calculate split sizes (80% train, 20% test)\n",
    "total_size = len(dataset_expert1)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset_expert1, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "logger.info(f\"\\nDataset split:\")\n",
    "logger.info(f\"Total samples: {total_size}\")\n",
    "logger.info(f\"Training samples: {len(train_dataset)}\")\n",
    "logger.info(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df1c53aa-5018-4f72-b9b7-9808defc7928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the weights based on the impacts:\n",
    "# Initialize model with clinical priorities\n",
    "# Weights for boundaries between classes (4 boundaries for 5 classes)\n",
    "class_weights = torch.tensor([2.0, 1.5, 1.5, 2.0])  # Adjusted for ordinal boundaries\n",
    "criterion = OrdinalRegressionLoss(num_classes=5, weights=class_weights)\n",
    "net = ImprovedNet(num_classes=5)\n",
    "optimizer = optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7de3c81b-bbf4-4233-b2ba-0e141ca45a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cpu')\n",
    "net = net.to(device)\n",
    "\n",
    "def evaluate_clinical_metrics(model, data_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Evaluate model with emphasis on clinical priorities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Initialize metrics\n",
    "    precision = Precision(task=\"multiclass\", num_classes=5, average=None).to(device)\n",
    "    recall = Recall(task=\"multiclass\", num_classes=5, average=None).to(device)\n",
    "    f1 = F1Score(task=\"multiclass\", num_classes=5, average=None).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Convert ordinal outputs to class predictions\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.sum(probs > 0.5, dim=1)\n",
    "            preds = torch.clamp(preds, 0, 4)  # Ensure predictions are within valid range\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            # Update metrics\n",
    "            precision(preds, labels)\n",
    "            recall(preds, labels)\n",
    "            f1(preds, labels)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Calculate Cohen's Kappa with quadratic weights\n",
    "    kappa = cohen_kappa_score(all_labels, all_preds, weights='quadratic')\n",
    "    \n",
    "    # Calculate Mean Absolute Error\n",
    "    mae = np.mean(np.abs(all_preds - all_labels))\n",
    "    \n",
    "    # Get per-class metrics\n",
    "    precision_values = precision.compute()\n",
    "    recall_values = recall.compute()\n",
    "    f1_values = f1.compute()\n",
    "    \n",
    "    # Create detailed metrics dictionary\n",
    "    metrics = {\n",
    "        'per_class_metrics': {\n",
    "            class_name: {\n",
    "                'precision': float(precision_values[i]),\n",
    "                'recall': float(recall_values[i]),\n",
    "                'f1': float(f1_values[i]),\n",
    "                'support': int(np.sum(all_labels == i))\n",
    "            } for i, class_name in enumerate(class_names)\n",
    "        },\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'normalized_confusion_matrix': cm_normalized.tolist(),\n",
    "        'cohen_kappa': float(kappa),\n",
    "        'mae': float(mae),\n",
    "        'clinical_priorities': {\n",
    "            'severe_recall': float(recall_values[4]),  # KL4/Severe\n",
    "            'normal_precision': float(precision_values[0]),  # KL0/Normal\n",
    "            'moderate_metrics': {\n",
    "                'recall': float(recall_values[3]),  # KL3/Moderate\n",
    "                'precision': float(precision_values[3])\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate Clinical Utility Score (weighted average of priority metrics)\n",
    "    clinical_utility = (\n",
    "        0.4 * metrics['clinical_priorities']['severe_recall'] +\n",
    "        0.3 * metrics['clinical_priorities']['normal_precision'] +\n",
    "        0.2 * metrics['clinical_priorities']['moderate_metrics']['recall'] +\n",
    "        0.1 * metrics['clinical_priorities']['moderate_metrics']['precision']\n",
    "    )\n",
    "    metrics['clinical_utility_score'] = float(clinical_utility)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "248accfc-b6fb-489b-be42-f6247ec6163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "def plot_confusion_matrix(cm, class_names, title='Confusion Matrix', normalize=False):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with emphasis on critical misclassifications\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'confusion_matrix{\"_normalized\" if normalize else \"\"}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07027322-697b-4cc7-a17d-3b3188ab0de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:00:21,521 - INFO - \n",
      "Starting training with clinical priorities...\n",
      "2025-05-18 18:00:43,337 - INFO - \n",
      "Epoch 1 Clinical Metrics:\n",
      "2025-05-18 18:00:43,338 - INFO - Clinical Utility Score: 0.098\n",
      "2025-05-18 18:00:43,338 - INFO - Severe (KL4) Recall: 0.000\n",
      "2025-05-18 18:00:43,338 - INFO - Normal (KL0) Precision: 0.328\n",
      "2025-05-18 18:00:43,504 - INFO - \n",
      "New best model saved with clinical utility score: 0.098\n",
      "2025-05-18 18:01:04,818 - INFO - \n",
      "Epoch 2 Clinical Metrics:\n",
      "2025-05-18 18:01:04,818 - INFO - Clinical Utility Score: 0.116\n",
      "2025-05-18 18:01:04,818 - INFO - Severe (KL4) Recall: 0.000\n",
      "2025-05-18 18:01:04,819 - INFO - Normal (KL0) Precision: 0.387\n",
      "2025-05-18 18:01:04,972 - INFO - \n",
      "New best model saved with clinical utility score: 0.116\n",
      "2025-05-18 18:01:26,047 - INFO - \n",
      "Epoch 3 Clinical Metrics:\n",
      "2025-05-18 18:01:26,047 - INFO - Clinical Utility Score: 0.110\n",
      "2025-05-18 18:01:26,048 - INFO - Severe (KL4) Recall: 0.000\n",
      "2025-05-18 18:01:26,048 - INFO - Normal (KL0) Precision: 0.368\n",
      "2025-05-18 18:01:47,019 - INFO - \n",
      "Epoch 4 Clinical Metrics:\n",
      "2025-05-18 18:01:47,019 - INFO - Clinical Utility Score: 0.160\n",
      "2025-05-18 18:01:47,019 - INFO - Severe (KL4) Recall: 0.000\n",
      "2025-05-18 18:01:47,020 - INFO - Normal (KL0) Precision: 0.000\n",
      "2025-05-18 18:01:47,327 - INFO - \n",
      "New best model saved with clinical utility score: 0.160\n",
      "2025-05-18 18:02:08,564 - INFO - \n",
      "Epoch 5 Clinical Metrics:\n",
      "2025-05-18 18:02:08,564 - INFO - Clinical Utility Score: 0.106\n",
      "2025-05-18 18:02:08,564 - INFO - Severe (KL4) Recall: 0.000\n",
      "2025-05-18 18:02:08,565 - INFO - Normal (KL0) Precision: 0.355\n",
      "2025-05-18 18:02:29,703 - INFO - \n",
      "Epoch 6 Clinical Metrics:\n",
      "2025-05-18 18:02:29,703 - INFO - Clinical Utility Score: 0.374\n",
      "2025-05-18 18:02:29,704 - INFO - Severe (KL4) Recall: 0.196\n",
      "2025-05-18 18:02:29,704 - INFO - Normal (KL0) Precision: 0.800\n",
      "2025-05-18 18:02:29,854 - INFO - \n",
      "New best model saved with clinical utility score: 0.374\n",
      "2025-05-18 18:02:51,500 - INFO - \n",
      "Epoch 7 Clinical Metrics:\n",
      "2025-05-18 18:02:51,500 - INFO - Clinical Utility Score: 0.298\n",
      "2025-05-18 18:02:51,500 - INFO - Severe (KL4) Recall: 0.500\n",
      "2025-05-18 18:02:51,501 - INFO - Normal (KL0) Precision: 0.000\n",
      "2025-05-18 18:03:13,460 - INFO - \n",
      "Epoch 8 Clinical Metrics:\n",
      "2025-05-18 18:03:13,460 - INFO - Clinical Utility Score: 0.361\n",
      "2025-05-18 18:03:13,461 - INFO - Severe (KL4) Recall: 0.870\n",
      "2025-05-18 18:03:13,461 - INFO - Normal (KL0) Precision: 0.000\n",
      "2025-05-18 18:03:35,215 - INFO - \n",
      "Epoch 9 Clinical Metrics:\n",
      "2025-05-18 18:03:35,217 - INFO - Clinical Utility Score: 0.288\n",
      "2025-05-18 18:03:35,217 - INFO - Severe (KL4) Recall: 0.087\n",
      "2025-05-18 18:03:35,217 - INFO - Normal (KL0) Precision: 0.515\n",
      "2025-05-18 18:03:56,376 - INFO - \n",
      "Epoch 10 Clinical Metrics:\n",
      "2025-05-18 18:03:56,377 - INFO - Clinical Utility Score: 0.341\n",
      "2025-05-18 18:03:56,377 - INFO - Severe (KL4) Recall: 0.196\n",
      "2025-05-18 18:03:56,377 - INFO - Normal (KL0) Precision: 0.625\n",
      "2025-05-18 18:04:17,170 - INFO - \n",
      "Epoch 11 Clinical Metrics:\n",
      "2025-05-18 18:04:17,171 - INFO - Clinical Utility Score: 0.287\n",
      "2025-05-18 18:04:17,171 - INFO - Severe (KL4) Recall: 0.152\n",
      "2025-05-18 18:04:17,171 - INFO - Normal (KL0) Precision: 0.525\n",
      "2025-05-18 18:04:37,753 - INFO - \n",
      "Epoch 12 Clinical Metrics:\n",
      "2025-05-18 18:04:37,753 - INFO - Clinical Utility Score: 0.377\n",
      "2025-05-18 18:04:37,753 - INFO - Severe (KL4) Recall: 0.717\n",
      "2025-05-18 18:04:37,754 - INFO - Normal (KL0) Precision: 0.000\n",
      "2025-05-18 18:04:37,904 - INFO - \n",
      "New best model saved with clinical utility score: 0.377\n",
      "2025-05-18 18:04:58,315 - INFO - \n",
      "Epoch 13 Clinical Metrics:\n",
      "2025-05-18 18:04:58,315 - INFO - Clinical Utility Score: 0.437\n",
      "2025-05-18 18:04:58,316 - INFO - Severe (KL4) Recall: 0.326\n",
      "2025-05-18 18:04:58,316 - INFO - Normal (KL0) Precision: 0.590\n",
      "2025-05-18 18:04:58,463 - INFO - \n",
      "New best model saved with clinical utility score: 0.437\n",
      "2025-05-18 18:05:18,784 - INFO - \n",
      "Epoch 14 Clinical Metrics:\n",
      "2025-05-18 18:05:18,785 - INFO - Clinical Utility Score: 0.472\n",
      "2025-05-18 18:05:18,785 - INFO - Severe (KL4) Recall: 0.391\n",
      "2025-05-18 18:05:18,785 - INFO - Normal (KL0) Precision: 0.588\n",
      "2025-05-18 18:05:18,931 - INFO - \n",
      "New best model saved with clinical utility score: 0.472\n",
      "2025-05-18 18:05:39,384 - INFO - \n",
      "Epoch 15 Clinical Metrics:\n",
      "2025-05-18 18:05:39,385 - INFO - Clinical Utility Score: 0.398\n",
      "2025-05-18 18:05:39,385 - INFO - Severe (KL4) Recall: 0.239\n",
      "2025-05-18 18:05:39,385 - INFO - Normal (KL0) Precision: 0.579\n",
      "2025-05-18 18:06:00,264 - INFO - \n",
      "Epoch 16 Clinical Metrics:\n",
      "2025-05-18 18:06:00,264 - INFO - Clinical Utility Score: 0.351\n",
      "2025-05-18 18:06:00,265 - INFO - Severe (KL4) Recall: 0.217\n",
      "2025-05-18 18:06:00,265 - INFO - Normal (KL0) Precision: 0.525\n",
      "2025-05-18 18:06:20,707 - INFO - \n",
      "Epoch 17 Clinical Metrics:\n",
      "2025-05-18 18:06:20,707 - INFO - Clinical Utility Score: 0.445\n",
      "2025-05-18 18:06:20,707 - INFO - Severe (KL4) Recall: 0.326\n",
      "2025-05-18 18:06:20,707 - INFO - Normal (KL0) Precision: 0.622\n",
      "2025-05-18 18:06:41,281 - INFO - \n",
      "Epoch 18 Clinical Metrics:\n",
      "2025-05-18 18:06:41,282 - INFO - Clinical Utility Score: 0.552\n",
      "2025-05-18 18:06:41,282 - INFO - Severe (KL4) Recall: 0.457\n",
      "2025-05-18 18:06:41,283 - INFO - Normal (KL0) Precision: 0.711\n",
      "2025-05-18 18:06:41,440 - INFO - \n",
      "New best model saved with clinical utility score: 0.552\n",
      "2025-05-18 18:07:09,065 - INFO - \n",
      "Epoch 19 Clinical Metrics:\n",
      "2025-05-18 18:07:09,066 - INFO - Clinical Utility Score: 0.515\n",
      "2025-05-18 18:07:09,066 - INFO - Severe (KL4) Recall: 0.370\n",
      "2025-05-18 18:07:09,067 - INFO - Normal (KL0) Precision: 0.659\n",
      "2025-05-18 18:07:39,611 - INFO - \n",
      "Epoch 20 Clinical Metrics:\n",
      "2025-05-18 18:07:39,612 - INFO - Clinical Utility Score: 0.478\n",
      "2025-05-18 18:07:39,612 - INFO - Severe (KL4) Recall: 0.304\n",
      "2025-05-18 18:07:39,613 - INFO - Normal (KL0) Precision: 0.656\n",
      "2025-05-18 18:07:39,613 - INFO - \n",
      "Training completed!\n",
      "2025-05-18 18:07:39,614 - INFO - Best clinical utility score: 0.552\n"
     ]
    }
   ],
   "source": [
    "# Training loop with clinical evaluation\n",
    "num_epochs = 20\n",
    "best_clinical_utility = 0\n",
    "logger.info(\"\\nStarting training with clinical priorities...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 50 == 49:\n",
    "            logger.info(f'[Epoch {epoch + 1}, Batch {batch_idx + 1}] loss: {running_loss / 50:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # Evaluation phase\n",
    "    metrics = evaluate_clinical_metrics(net, test_loader, device, dataset_expert1.classes)\n",
    "    \n",
    "    # Log results\n",
    "    logger.info(f'\\nEpoch {epoch + 1} Clinical Metrics:')\n",
    "    logger.info(f'Clinical Utility Score: {metrics[\"clinical_utility_score\"]:.3f}')\n",
    "    logger.info(f'Severe (KL4) Recall: {metrics[\"clinical_priorities\"][\"severe_recall\"]:.3f}')\n",
    "    logger.info(f'Normal (KL0) Precision: {metrics[\"clinical_priorities\"][\"normal_precision\"]:.3f}')\n",
    "    \n",
    "    # Update learning rate based on clinical utility score\n",
    "    scheduler.step(metrics['clinical_utility_score'])\n",
    "    \n",
    "    # Save best model based on clinical utility\n",
    "    if metrics['clinical_utility_score'] > best_clinical_utility:\n",
    "        best_clinical_utility = metrics['clinical_utility_score']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            'clinical_utility_score': best_clinical_utility\n",
    "        }, 'best_clinical_model.pth')\n",
    "        \n",
    "        # Save detailed metrics to JSON\n",
    "        with open('best_model_metrics.json', 'w') as f:\n",
    "            json.dump(metrics, f, indent=4)\n",
    "        \n",
    "        # Plot confusion matrices\n",
    "        plot_confusion_matrix(np.array(metrics['confusion_matrix']), \n",
    "                            dataset_expert1.classes, \n",
    "                            title='Best Model Confusion Matrix')\n",
    "        plot_confusion_matrix(np.array(metrics['normalized_confusion_matrix']), \n",
    "                            dataset_expert1.classes, \n",
    "                            title='Best Model Normalized Confusion Matrix',\n",
    "                            normalize=True)\n",
    "        \n",
    "        logger.info(f'\\nNew best model saved with clinical utility score: {best_clinical_utility:.3f}')\n",
    "\n",
    "logger.info(\"\\nTraining completed!\")\n",
    "logger.info(f\"Best clinical utility score: {best_clinical_utility:.3f}\")\n",
    "\n",
    "#### END OF TRAINING AND EVALUATION ############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5266aec1-1f96-4083-b2b5-a3b2f2df3ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 18:19:37,917 - INFO - \n",
      "Evaluating model metrics on test set...\n",
      "2025-05-18 18:19:40,082 - INFO - \n",
      "Test Set Evaluation Metrics:\n",
      "2025-05-18 18:19:40,082 - INFO - \n",
      "Test Micro-averaging metrics:\n",
      "2025-05-18 18:19:40,082 - INFO - Precision: 0.309\n",
      "2025-05-18 18:19:40,083 - INFO - Recall: 0.309\n",
      "2025-05-18 18:19:40,083 - INFO - \n",
      "Test Macro-averaging metrics:\n",
      "2025-05-18 18:19:40,083 - INFO - Precision: 0.062\n",
      "2025-05-18 18:19:40,083 - INFO - Recall: 0.200\n",
      "2025-05-18 18:19:40,084 - INFO - \n",
      "Test Per-class metrics:\n",
      "2025-05-18 18:19:40,084 - INFO - \n",
      "Class: 0Normal\n",
      "2025-05-18 18:19:40,084 - INFO - Precision: 0.309\n",
      "2025-05-18 18:19:40,084 - INFO - Recall: 1.000\n",
      "2025-05-18 18:19:40,084 - INFO - \n",
      "Class: 1Doubtful\n",
      "2025-05-18 18:19:40,085 - INFO - Precision: 0.000\n",
      "2025-05-18 18:19:40,085 - INFO - Recall: 0.000\n",
      "2025-05-18 18:19:40,085 - INFO - \n",
      "Class: 2Mild\n",
      "2025-05-18 18:19:40,085 - INFO - Precision: 0.000\n",
      "2025-05-18 18:19:40,086 - INFO - Recall: 0.000\n",
      "2025-05-18 18:19:40,086 - INFO - \n",
      "Class: 3Moderate\n",
      "2025-05-18 18:19:40,086 - INFO - Precision: 0.000\n",
      "2025-05-18 18:19:40,086 - INFO - Recall: 0.000\n",
      "2025-05-18 18:19:40,086 - INFO - \n",
      "Class: 4Severe\n",
      "2025-05-18 18:19:40,087 - INFO - Precision: 0.000\n",
      "2025-05-18 18:19:40,087 - INFO - Recall: 0.000\n"
     ]
    }
   ],
   "source": [
    "###### Model Evaluation Metrics #####################################################################\n",
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "def evaluate_test_metrics(net, test_loader, device, num_classes=5):\n",
    "    \"\"\"\n",
    "    Evaluate model performance on test set using precision and recall metrics\n",
    "    \"\"\"\n",
    "    # Set model to evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Initialize metrics for test set evaluation\n",
    "    test_precision_micro = Precision(task=\"multiclass\", num_classes=num_classes, average=\"micro\").to(device)\n",
    "    test_precision_macro = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
    "    test_precision_none = Precision(task=\"multiclass\", num_classes=num_classes, average=None).to(device)\n",
    "    \n",
    "    test_recall_micro = Recall(task=\"multiclass\", num_classes=num_classes, average=\"micro\").to(device)\n",
    "    test_recall_macro = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\").to(device)\n",
    "    test_recall_none = Recall(task=\"multiclass\", num_classes=num_classes, average=None).to(device)\n",
    "    \n",
    "    # Evaluate on test set only\n",
    "    with torch.no_grad():\n",
    "        for test_images, test_labels in test_loader:\n",
    "            test_images = test_images.to(device)\n",
    "            test_labels = test_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            test_outputs = net(test_images)\n",
    "            _, test_preds = torch.max(test_outputs, 1)\n",
    "            \n",
    "            # Update metrics\n",
    "            test_precision_micro(test_preds, test_labels)\n",
    "            test_precision_macro(test_preds, test_labels)\n",
    "            test_precision_none(test_preds, test_labels)\n",
    "            \n",
    "            test_recall_micro(test_preds, test_labels)\n",
    "            test_recall_macro(test_preds, test_labels)\n",
    "            test_recall_none(test_preds, test_labels)\n",
    "    \n",
    "    # Compute final test metrics\n",
    "    test_micro_precision = test_precision_micro.compute()\n",
    "    test_macro_precision = test_precision_macro.compute()\n",
    "    test_none_precision = test_precision_none.compute()\n",
    "    \n",
    "    test_micro_recall = test_recall_micro.compute()\n",
    "    test_macro_recall = test_recall_macro.compute()\n",
    "    test_none_recall = test_recall_none.compute()\n",
    "    \n",
    "    # Log test results\n",
    "    logger.info(\"\\nTest Set Evaluation Metrics:\")\n",
    "    \n",
    "    logger.info(\"\\nTest Micro-averaging metrics:\")\n",
    "    logger.info(f\"Precision: {test_micro_precision:.3f}\")\n",
    "    logger.info(f\"Recall: {test_micro_recall:.3f}\")\n",
    "    \n",
    "    logger.info(\"\\nTest Macro-averaging metrics:\")\n",
    "    logger.info(f\"Precision: {test_macro_precision:.3f}\")\n",
    "    logger.info(f\"Recall: {test_macro_recall:.3f}\")\n",
    "    \n",
    "    logger.info(\"\\nTest Per-class metrics:\")\n",
    "    for i, class_name in enumerate(dataset_expert1.classes):\n",
    "        logger.info(f\"\\nClass: {class_name}\")\n",
    "        logger.info(f\"Precision: {test_none_precision[i]:.3f}\")\n",
    "        logger.info(f\"Recall: {test_none_recall[i]:.3f}\")\n",
    "\n",
    "# Run evaluation on test set\n",
    "logger.info(\"\\nEvaluating model metrics on test set...\")\n",
    "evaluate_test_metrics(net, test_loader, device)\n",
    "\n",
    "#### END OF MODEL EVALUATION METRICS ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0432f6-f11c-424f-a9a5-f9f0a9582006",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################ END OF THE SCRIPT ##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
